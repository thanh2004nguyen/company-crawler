"""
Northdata.de Scraper
Scrapes business data from Northdata using Playwright
"""

import os
import sys
import time
import logging
import io
from typing import Dict, Optional, List
from playwright.sync_api import sync_playwright, Page, Browser

# Add project root to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Force UTF-8 encoding cho console
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except AttributeError:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=True)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NorthdataScraper:
    """Scraper for northdata.de using Playwright"""
    
    def __init__(self, headless: bool = False):
        self.base_url = "https://www.northdata.de"
        self.headless = headless
        
        logger.info("üåê Northdata Scraper initialized")
    
    def scrape_company(self, company_name: str, registernummer: str) -> Dict:
        """
        Scrape company data from northdata.de
        
        Args:
            company_name: Company name
            registernummer: HRB number
            
        Returns:
            Dict with scraped data
        """
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=self.headless)
            page = browser.new_page()
            
            try:
                logger.info(f"üîç Searching Northdata for: {company_name}")
                
                # Navigate to Northdata
                page.goto(self.base_url, wait_until='networkidle')
                logger.info("‚úÖ ƒê√£ truy c·∫≠p Northdata")
                
                # Handle cookie consent popup
                try:
                    cookie_popup = page.locator('text="Accept all"').first
                    is_visible = cookie_popup.is_visible(timeout=3000)
                    if is_visible:
                        cookie_popup.click()
                        logger.info("üç™ ƒê√£ accept cookie consent")
                        page.wait_for_timeout(1000)  # Wait for popup to disappear
                except:
                    logger.info("‚ÑπÔ∏è Kh√¥ng c√≥ cookie popup ho·∫∑c ƒë√£ ƒë∆∞·ª£c handle")
                
                # Fill search box with company name only
                search_box = page.locator('input[name="query"]')
                search_box.fill(company_name)
                logger.info(f"üìù ƒê√£ nh·∫≠p t√™n c√¥ng ty: {company_name}")
                
                # Press Enter to search with longer timeout
                search_box.press('Enter', timeout=60000)
                logger.info("üîç ƒê√£ b·∫•m Enter ƒë·ªÉ search")
                
                # ƒê·ª£i sau khi search
                page.wait_for_timeout(5000)  # ƒê·ª£i 5 gi√¢y cho trang load
                logger.info("‚è≥ ƒê√£ ƒë·ª£i 5 gi√¢y sau khi search")
                
                # Check current URL
                current_url = page.url
                logger.info(f"üìç Current URL: {current_url}")
                
                # Ki·ªÉm tra xem c√≥ ph·∫£i ƒë√£ ·ªü company page kh√¥ng b·∫±ng c√°ch t√¨m heading
                heading_span = page.locator('span.heading').first
                if heading_span and heading_span.is_visible():
                    heading_text = heading_span.inner_text()
                    logger.info(f"üéØ T√¨m th·∫•y heading: {heading_text}")
                    
                    # Ki·ªÉm tra xem heading c√≥ ch·ª©a t√™n c√¥ng ty kh√¥ng
                    if company_name.lower() in heading_text.lower():
                        logger.info("‚úÖ ƒê√£ ·ªü ƒë√∫ng company page, kh√¥ng c·∫ßn click th√™m")
                        # ƒê√£ ·ªü company page r·ªìi, kh√¥ng c·∫ßn t√¨m search results
                    else:
                        logger.warning(f"‚ö†Ô∏è Heading kh√¥ng kh·ªõp v·ªõi company name: {company_name}")
                        # Fallback: t√¨m trong search results
                        try:
                            results = page.locator('.event')
                            result_count = results.count()
                            logger.info(f"üìä T√¨m th·∫•y {result_count} k·∫øt qu·∫£")
                            
                            if result_count > 0:
                                first_result = results.first
                                first_result.click()
                                logger.info("‚úÖ ƒê√£ click v√†o k·∫øt qu·∫£ ƒë·∫ßu ti√™n")
                                page.wait_for_timeout(3000)
                        except Exception as e:
                            logger.error(f"‚ùå Kh√¥ng th·ªÉ click v√†o k·∫øt qu·∫£: {e}")
                else:
                    logger.info("üîç Kh√¥ng t√¨m th·∫•y heading, c√≥ th·ªÉ v·∫´n ·ªü search results page")
                    # T√¨m v√† click v√†o c√¥ng ty c√≥ s·ªë ƒëƒÉng k√Ω kh·ªõp t·ª´ search results
                    try:
                        results = page.locator('.event')
                        result_count = results.count()
                        logger.info(f"üìä T√¨m th·∫•y {result_count} k·∫øt qu·∫£")
                        
                        if result_count > 0:
                            first_result = results.first
                            first_result.click()
                            logger.info("‚úÖ ƒê√£ click v√†o k·∫øt qu·∫£ ƒë·∫ßu ti√™n")
                            page.wait_for_timeout(3000)
                        else:
                            logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o")
                    except Exception as e:
                        logger.error(f"‚ùå Kh√¥ng th·ªÉ t√¨m ho·∫∑c click v√†o c√¥ng ty: {e}")
                        return {
                            "company_name": company_name,
                            "registernummer": registernummer,
                            "error": f"Kh√¥ng th·ªÉ t√¨m ho·∫∑c click v√†o c√¥ng ty: {e}"
                        }
                
                # ƒê·ª£i page load ho√†n to√†n
                page.wait_for_timeout(3000)
                
                # Ki·ªÉm tra xem c√≥ ph·∫£i Premium content kh√¥ng
                page_content = page.content()
                if "nicht √∂ffentlich verf√ºgbar" in page_content or "Premium Service" in page_content:
                    logger.warning("‚ö†Ô∏è Company data requires Premium Service, ch·ªâ l·∫•y HTML c√≥ s·∫µn")
                
                # L∆∞u HTML v√†o th∆∞ m·ª•c data/companies/ v√† l·∫•y filepath
                html_filepath = self._save_html_to_magna_folder(page, company_name, registernummer)
                
                # Extract data t·ª´ company page
                data = self._extract_company_data(page, company_name, registernummer)
                
                # Th√™m HTML filepath v√†o data
                data['html_filepath'] = html_filepath
                
                logger.info(f"‚úÖ ƒê√£ extract {len(data)} tr∆∞·ªùng t·ª´ Northdata")
                return data
                    
            except Exception as e:
                logger.error(f"‚ùå L·ªói scrape Northdata: {str(e)}")
                return {}
            finally:
                browser.close()
    
    def _find_company_link(self, page: Page, registernummer: str) -> Optional[any]:
        """T√¨m company link d·ª±a tr√™n registernummer"""
        try:
            # Look for company links in search results with multiple selectors
            # Based on northdata.de structure
            selectors_to_try = [
                '.event[data-uri]',
                '.ui.card',
                '.search-result',
                '.company-result',
                '.ui.items .item',  # Northdata uses Semantic UI
                '.result-item',
                'a[href*="/"]'  # Any link that might be a company
            ]
            
            for selector in selectors_to_try:
                try:
                    logger.info(f"üîç T√¨m ki·∫øm v·ªõi selector: {selector}")
                    company_events = page.locator(selector)
                    count = company_events.count()
                    logger.info(f"üìä T√¨m th·∫•y {count} elements v·ªõi selector {selector}")
                    
                    for i in range(count):
                        event = company_events.nth(i)
                        
                        # Try multiple ways to get text content
                        text_sources = [
                            event.locator('.extra.text'),
                            event.locator('.meta'),
                            event.locator('.description'),
                            event.locator('.content'),
                            event.locator('a'),
                            event
                        ]
                        
                        for text_source in text_sources:
                            try:
                                extra_text = text_source.text_content()
                                if extra_text:
                                    logger.info(f"üìù Text content: {extra_text[:100]}...")
                                    
                                    if registernummer in extra_text:
                                        # Found matching company - try different link selectors
                                        link_selectors = ['a.title', 'a', '.title a', 'h3 a', 'h2 a']
                                        for link_selector in link_selectors:
                                            try:
                                                company_link = event.locator(link_selector).first
                                                if company_link.is_visible():
                                                    logger.info(f"üéØ T√¨m th·∫•y match: {extra_text[:50]}...")
                                                    return company_link
                                            except:
                                                continue
                                        
                                        # If no specific link found, try the event itself if it's clickable
                                        try:
                                            if event.locator('a').count() > 0:
                                                return event.locator('a').first
                                        except:
                                            pass
                            except:
                                continue
                                
                except Exception as e:
                    logger.info(f"‚ö†Ô∏è L·ªói v·ªõi selector {selector}: {str(e)}")
                    continue
            
            logger.warning(f"‚ùå Kh√¥ng t√¨m th·∫•y company v·ªõi HRB: {registernummer}")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói find company link: {str(e)}")
            return None
    
    def _extract_company_data(self, page: Page, company_name: str, registernummer: str) -> Dict:
        """Extract data t·ª´ company page - CH·ªà l·∫•y c√°c tr∆∞·ªùng trong CompanyData model"""
        try:
            # CH·ªà extract c√°c tr∆∞·ªùng c√≥ trong CompanyData model (27 tr∆∞·ªùng)
            data = {
                'registernummer': registernummer,
                # Basic info
                'handelsregister': self._extract_handelsregister(page),
                'geschaeftsadresse': self._extract_geschaeftsadresse(page),
                'unternehmenszweck': self._extract_unternehmenszweck(page),
                'land_des_hauptsitzes': self._extract_land_des_hauptsitzes(page),
                'gerichtsstand': self._extract_gerichtsstand(page),
                'paragraph_34_gewo': self._extract_paragraph_34_gewo(page),
                
                # Financial data
                'mitarbeiter': self._extract_mitarbeiter(page),
                'umsatz': self._extract_umsatz(page), 
                'gewinn': self._extract_gewinn(page),
                'insolvenz': self._extract_insolvenz(page),
                
                # Real estate data
                'anzahl_immobilien': self._extract_anzahl_immobilien(page),
                'gesamtwert_immobilien': self._extract_gesamtwert_immobilien(page),
                
                # Other data
                'sonstige_rechte': self._extract_sonstige_rechte(page),
                'gruendungsdatum': self._extract_gruendungsdatum(page),
                'aktiv_seit': self._extract_aktiv_seit(page),
                
                # Contact info
                'geschaeftsfuehrer': self._extract_geschaeftsfuehrer(page),
                'telefonnummer': self._extract_telefonnummer(page),
                'email': self._extract_email(page),
                'website': self._extract_website(page)
            }
            
            # Remove None values
            data = {k: v for k, v in data.items() if v is not None}
            
            logger.info(f"‚úÖ Northdata extract: {len(data)} tr∆∞·ªùng trong model")
            return data
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract company data: {str(e)}")
            return {}
    
    def _extract_mitarbeiter(self, page: Page) -> Optional[int]:
        """Extract s·ªë l∆∞·ª£ng nh√¢n vi√™n t·ª´ bi·ªÉu ƒë·ªì/charts"""
        try:
            # Look for employee count in various possible locations
            # Based on northdata.de structure with charts
            selectors = [
                'text=/\\d+\\s*Mitarbeiter/',
                'text=/\\d+\\s*employees/',
                '[data-testid="employees"]',
                '.employee-count',
                '.mitarbeiter',
                # Northdata specific selectors
                'text=/MITARBEITER/',
                '.chart-container',
                '.financial-data',
                '.metric-value'
            ]
            
            # First try to find in chart tabs or financial data
            page_content = page.content()
            
            # Look for MITARBEITER tab or section
            if 'MITARBEITER' in page_content:
                logger.info("üéØ T√¨m th·∫•y MITARBEITER section")
                # Try to find the actual value in the chart or data
                import re
                # Look for patterns like "14 Mitarbeiter" or just numbers
                mitarbeiter_patterns = [
                    r'(\d+)\s*Mitarbeiter',
                    r'(\d+)\s*employees',
                    r'MITARBEITER.*?(\d+)',
                    r'(\d+).*?Mitarbeiter'
                ]
                
                for pattern in mitarbeiter_patterns:
                    matches = re.findall(pattern, page_content, re.IGNORECASE)
                    if matches:
                        try:
                            return int(matches[0])
                        except:
                            continue
            
            # Fallback: try element selectors
            for selector in selectors:
                try:
                    element = page.locator(selector).first
                    is_visible = element.is_visible()
                    if is_visible:
                        text = element.text_content()
                        # Extract number from text
                        import re
                        numbers = re.findall(r'\d+', text)
                        if numbers:
                            return int(numbers[0])
                except:
                    continue
            
            logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y s·ªë l∆∞·ª£ng nh√¢n vi√™n")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract mitarbeiter: {str(e)}")
            return None
    
    def _extract_umsatz(self, page: Page) -> Optional[float]:
        """Extract doanh thu (revenue) t·ª´ bi·ªÉu ƒë·ªì UMS√ÑTZ"""
        try:
            # Look for revenue data in financial charts or tables
            # Based on northdata.de structure with UMS√ÑTZ tab
            page_content = page.content()
            
            # Look for UMS√ÑTZ tab or section (with √Ñ character)
            if 'UMS√ÑTZ' in page_content or 'UMSATZ' in page_content:
                logger.info("üéØ T√¨m th·∫•y UMS√ÑTZ section")
                import re
                
                # Look for revenue patterns in German format
                umsatz_patterns = [
                    r'(\d+)[.,](\d+)\s*Mio\\.?\s*‚Ç¨',  # "24,1 Mio. ‚Ç¨"
                    r'(\d+)\s*Mio\\.?\s*‚Ç¨',           # "24 Mio. ‚Ç¨"
                    r'(\d+[.,]\d+)\s*Mio',            # "24,1 Mio"
                    r'(\d+)\s*Millionen',             # "24 Millionen"
                    r'UMS√ÑTZ.*?(\d+[.,]\d+)',        # "UMS√ÑTZ 24,1"
                    r'(\d+[.,]\d+).*?Mio.*?‚Ç¨'        # Various formats
                ]
                
                for pattern in umsatz_patterns:
                    matches = re.findall(pattern, page_content, re.IGNORECASE)
                    if matches:
                        try:
                            if isinstance(matches[0], tuple):
                                whole, decimal = matches[0]
                                return float(f"{whole}.{decimal}")
                            else:
                                num_str = str(matches[0]).replace(',', '.')
                                return float(num_str)
                        except:
                            continue
            
            # Fallback: try element selectors
            selectors = [
                'text=/Umsatz/',
                'text=/Revenue/',
                'text=/\\d+[.,]\\d+\\s*Mio\\.?\\s*‚Ç¨/',
                '[data-testid="revenue"]',
                '.umsatz',
                '.revenue',
                '.chart-container',
                '.financial-data'
            ]
            
            for selector in selectors:
                try:
                    element = page.locator(selector).first
                    if element.is_visible():
                        text = element.text_content()
                        # Extract number from German format
                        import re
                        numbers = re.findall(r'(\d+)[.,](\d+)\s*Mio', text)
                        if numbers:
                            whole, decimal = numbers[0]
                            return float(f"{whole}.{decimal}")
                        
                        # Try simple number extraction
                        numbers = re.findall(r'(\d+[.,]\d+)', text)
                        if numbers:
                            num_str = numbers[0].replace(',', '.')
                            return float(num_str)
                except:
                    continue
            
            logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y doanh thu")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract umsatz: {str(e)}")
            return None
    
    def _extract_gewinn(self, page: Page) -> Optional[float]:
        """Extract l·ª£i nhu·∫≠n (profit/loss) t·ª´ bi·ªÉu ƒë·ªì GEWINN"""
        try:
            # Look for profit/loss data in GEWINN tab
            page_content = page.content()
            
            # Look for GEWINN tab or section
            if 'GEWINN' in page_content:
                logger.info("üéØ T√¨m th·∫•y GEWINN section")
                import re
                
                # Look for profit/loss patterns
                gewinn_patterns = [
                    r'(\d+)[.,](\d+)\s*Mio\\.?\s*‚Ç¨',  # "2,1 Mio. ‚Ç¨"
                    r'(\d+)\s*Mio\\.?\s*‚Ç¨',           # "2 Mio. ‚Ç¨"
                    r'-(\d+)[.,](\d+)\s*Mio\\.?\s*‚Ç¨', # "-2,1 Mio. ‚Ç¨" (loss)
                    r'GEWINN.*?(\d+[.,]\d+)',        # "GEWINN 2,1"
                    r'VERLUST.*?(\d+[.,]\d+)',       # "VERLUST 2,1"
                    r'(\d+[.,]\d+).*?Mio.*?‚Ç¨'        # Various formats
                ]
                
                for pattern in gewinn_patterns:
                    matches = re.findall(pattern, page_content, re.IGNORECASE)
                    if matches:
                        try:
                            if isinstance(matches[0], tuple):
                                whole, decimal = matches[0]
                                value = float(f"{whole}.{decimal}")
                            else:
                                num_str = str(matches[0]).replace(',', '.')
                                value = float(num_str)
                            
                            # Check if it's a loss (negative)
                            is_loss = 'VERLUST' in page_content or 'Verlust' in page_content or pattern.startswith('-')
                            return -value if is_loss else value
                        except:
                            continue
            
            # Fallback: try element selectors
            selectors = [
                'text=/Gewinn/',
                'text=/Verlust/',
                'text=/Profit/',
                'text=/Loss/',
                '[data-testid="profit"]',
                '.gewinn',
                '.profit',
                '.chart-container',
                '.financial-data'
            ]
            
            for selector in selectors:
                try:
                    element = page.locator(selector).first
                    if element.is_visible():
                        text = element.text_content()
                        
                        # Check if it's a loss (negative)
                        is_loss = 'Verlust' in text or 'Loss' in text or '-' in text
                        
                        # Extract number
                        import re
                        numbers = re.findall(r'(\d+[.,]\d+)', text)
                        if numbers:
                            num_str = numbers[0].replace(',', '.')
                            value = float(num_str)
                            return -value if is_loss else value
                except:
                    continue
            
            logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y l·ª£i nhu·∫≠n")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract gewinn: {str(e)}")
            return None
    
    def _extract_insolvenz(self, page: Page) -> Optional[bool]:
        """Extract tr·∫°ng th√°i ph√° s·∫£n"""
        try:
            # Look for insolvency indicators
            insolvency_indicators = [
                '‚úùÔ∏é',  # Death symbol used for terminated companies
                'Liquidation',
                'Insolvenz',
                'Insolvency',
                'Erloschen',
                'Terminiert'
            ]
            
            page_content = page.content()
            
            for indicator in insolvency_indicators:
                if indicator in page_content:
                    logger.info(f"üö® Ph√°t hi·ªán ch·ªâ s·ªë ph√° s·∫£n: {indicator}")
                    return True
            
            logger.info("‚úÖ Company kh√¥ng c√≥ d·∫•u hi·ªáu ph√° s·∫£n")
            return False
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract insolvenz: {str(e)}")
            return None
    
    
    def _extract_handelsregister(self, page: Page) -> Optional[str]:
        """Extract Handelsregister t·ª´ page"""
        try:
            page_content = page.content()
            import re
            
            # Pattern: "Amtsgericht Hamburg HRB"
            pattern = r'Amtsgericht\s+(\w+)'
            match = re.search(pattern, page_content)
            
            if match:
                city = match.group(1)
                logger.info(f"üéØ T√¨m th·∫•y Handelsregister: {city}")
                return city
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract handelsregister: {str(e)}")
            return None
    
    def _extract_geschaeftsadresse(self, page: Page) -> Optional[str]:
        """Extract Gesch√§ftsadresse t·ª´ page"""
        try:
            page_content = page.content()
            import re
            
            # Pattern: "Gro√üe Elbstr. 61, D-22767 Hamburg"
            pattern = r'Gro√üe Elbstr[^,]+,\s*D-\d+\s+\w+'
            match = re.search(pattern, page_content)
            
            if match:
                address = match.group(0)
                logger.info(f"üéØ T√¨m th·∫•y Gesch√§ftsadresse: {address}")
                return address
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract geschaeftsadresse: {str(e)}")
            return None
    
    def _extract_unternehmenszweck(self, page: Page) -> Optional[str]:
        """Extract Unternehmenszweck t·ª´ page content"""
        try:
            page_content = page.content()
            import re
            
            # T√¨m pattern "Gegenstand des Unternehmens"
            pattern = r'Gegenstand des Unternehmens der Gesellschaft ist ([^<]+)'
            match = re.search(pattern, page_content)
            
            if match:
                zweck = match.group(1).strip()
                logger.info(f"üéØ T√¨m th·∫•y Unternehmenszweck: {zweck[:50]}...")
                return zweck
            
            logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Unternehmenszweck")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract unternehmenszweck: {str(e)}")
            return None
    
    def _extract_land_des_hauptsitzes(self, page: Page) -> Optional[str]:
        """Extract Land des Hauptsitzes t·ª´ ƒë·ªãa ch·ªâ"""
        try:
            page_content = page.content()
            import re
            
            # T√¨m pattern "D-xxxxx" (D = Deutschland)
            pattern = r'\bD-\d{5}\b'
            match = re.search(pattern, page_content)
            
            if match:
                logger.info(f"üéØ T√¨m th·∫•y Land: Deutschland (t·ª´ D-xxxxx)")
                return "Deutschland"
            
            # Fallback: T√¨m "Deutschland" tr·ª±c ti·∫øp
            if 'Deutschland' in page_content:
                logger.info(f"üéØ T√¨m th·∫•y Land: Deutschland")
                return "Deutschland"
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract land_des_hauptsitzes: {str(e)}")
            return None
    
    def _extract_gerichtsstand(self, page: Page) -> Optional[str]:
        """Extract Gerichtsstand"""
        try:
            page_content = page.content()
            import re
            
            # Pattern: "Amtsgericht Hamburg"
            pattern = r'(Amtsgericht\s+\w+)'
            match = re.search(pattern, page_content)
            
            if match:
                gerichtsstand = match.group(1)
                logger.info(f"üéØ T√¨m th·∫•y Gerichtsstand: {gerichtsstand}")
                return gerichtsstand
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract gerichtsstand: {str(e)}")
            return None
    
    def _extract_paragraph_34_gewo(self, page: Page) -> Optional[bool]:
        """Extract ¬ß34 GewO status"""
        try:
            page_content = page.content()
            
            # T√¨m "¬ß 34c GewO" ho·∫∑c "¬ß34c GewO"
            if '¬ß 34c GewO' in page_content or '¬ß34c GewO' in page_content:
                logger.info(f"üéØ T√¨m th·∫•y ¬ß34c GewO: Ja")
                return True
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract paragraph_34_gewo: {str(e)}")
            return None
    
    def _extract_anzahl_immobilien(self, page: Page) -> Optional[int]:
        """Extract s·ªë l∆∞·ª£ng b·∫•t ƒë·ªông s·∫£n t·ª´ Northdata"""
        try:
            page_content = page.content()
            import re
            
            # T√¨m trong "Immobilien und Grundst√ºcke" section
            if 'Immobilien und Grundst√ºcke' in page_content:
                logger.info("üéØ T√¨m th·∫•y Immobilien section nh∆∞ng kh√¥ng c√≥ s·ªë l∆∞·ª£ng c·ª• th·ªÉ")
                # Northdata kh√¥ng cung c·∫•p s·ªë l∆∞·ª£ng c·ª• th·ªÉ, ch·ªâ c√≥ t·ªïng gi√° tr·ªã
                return None
            
            logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Immobilien section")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract anzahl_immobilien: {str(e)}")
            return None
    
    def _extract_gesamtwert_immobilien(self, page: Page) -> Optional[float]:
        """Extract t·ªïng gi√° tr·ªã b·∫•t ƒë·ªông s·∫£n t·ª´ Northdata"""
        try:
            page_content = page.content()
            import re
            
            # T√¨m "Finanzanlagen" c√≥ th·ªÉ coi l√† gi√° tr·ªã BƒêS
            pattern = r'(\d+[.,]\d+)\s*Mio\.\s*‚Ç¨.*?Finanzanlagen'
            match = re.search(pattern, page_content)
            
            if match:
                value = float(match.group(1).replace(',', '.'))
                logger.info(f"üéØ T√¨m th·∫•y Gesamtwert Immobilien (Finanzanlagen): {value} Mio. ‚Ç¨")
                return value
            
            logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Gesamtwert Immobilien")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract gesamtwert_immobilien: {str(e)}")
            return None
    
    def _extract_sonstige_rechte(self, page: Page) -> Optional[list]:
        """Extract Sonstige Rechte (LEI Code, trademarks, etc)"""
        try:
            page_content = page.content()
            import re
            
            rechte = []
            
            # LEI Code
            lei_pattern = r'([A-Z0-9]{20})'
            lei_match = re.search(lei_pattern, page_content)
            if lei_match:
                rechte.append(f"LEI: {lei_match.group(1)}")
            
            # Trademarks (Wortmarke, Wort-/Bildmarke)
            if 'Wortmarke' in page_content or 'Bildmarke' in page_content:
                trademark_pattern = r'(Wort-?/Bildmarke|Wortmarke):\s*["\']([^"\']+)["\']'
                trademark_matches = re.findall(trademark_pattern, page_content)
                for match in trademark_matches:
                    rechte.append(f"Trademark: {match[1]}")
            
            if rechte:
                logger.info(f"üéØ T√¨m th·∫•y {len(rechte)} Sonstige Rechte")
                return rechte
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract sonstige_rechte: {str(e)}")
            return None
    
    def _extract_gruendungsdatum(self, page: Page) -> Optional[str]:
        """Extract Gr√ºndungsdatum t·ª´ JSON-LD schema"""
        try:
            page_content = page.content()
            import re
            
            # CHU·∫®N NH·∫§T: T√¨m t·ª´ JSON-LD schema
            # Pattern: "foundingDate" : "2016-05-17"
            json_ld_pattern = r'"foundingDate"\s*:\s*"(\d{4}-\d{2}-\d{2})"'
            json_ld_match = re.search(json_ld_pattern, page_content)
            
            if json_ld_match:
                founding_date = json_ld_match.group(1)
                logger.info(f"üéØ T√¨m th·∫•y Gr√ºndungsdatum t·ª´ JSON-LD: {founding_date}")
                return founding_date
            
            # Fallback: T√¨m t·ª´ chart data "date" : "2016-05-17", "desc" : "...Eintragung"
            chart_pattern = r'"date"\s*:\s*"(\d{4}-\d{2}-\d{2})"\s*,\s*"desc"\s*:\s*"[^"]*Eintragung"'
            chart_match = re.search(chart_pattern, page_content)
            
            if chart_match:
                founding_date = chart_match.group(1)
                logger.info(f"üéØ T√¨m th·∫•y Gr√ºndungsdatum t·ª´ chart Eintragung: {founding_date}")
                return founding_date
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract gruendungsdatum: {str(e)}")
            return None
    
    def _extract_aktiv_seit(self, page: Page) -> Optional[str]:
        """Extract Aktiv seit - T√≠nh t·ª´ nƒÉm th√†nh l·∫≠p"""
        try:
            gruendungsdatum = self._extract_gruendungsdatum(page)
            
            if gruendungsdatum:
                from datetime import datetime
                current_year = datetime.now().year
                
                # Extract year t·ª´ date format YYYY-MM-DD ho·∫∑c YYYY
                if '-' in gruendungsdatum:
                    founding_year = int(gruendungsdatum.split('-')[0])
                else:
                    founding_year = int(gruendungsdatum)
                
                years_active = current_year - founding_year
                aktiv_seit = f"{years_active} Jahre"
                logger.info(f"üéØ Aktiv seit: {aktiv_seit}")
                return aktiv_seit
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract aktiv_seit: {str(e)}")
            return None
    
    def _extract_geschaeftsfuehrer(self, page: Page) -> Optional[list]:
        """Extract Gesch√§ftsf√ºhrer t·ª´ Netzwerk section"""
        try:
            page_content = page.content()
            import re
            
            geschaeftsfuehrer = []
            
            # T√¨m t√™n trong Netzwerk section (Martin G√∂cks, David Liebig, etc)
            # Pattern: T√™n ng∆∞·ªùi (2 t·ª´, ch·ªØ c√°i ƒë·∫ßu vi·∫øt hoa)
            pattern = r'([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)\s+([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+)'
            matches = re.findall(pattern, page_content)
            
            # Filter ra c√°c t√™n c√≥ v·∫ª l√† ng∆∞·ªùi (kh√¥ng ph·∫£i t√™n c√¥ng ty)
            known_names = ['Martin G√∂cks', 'David Liebig', 'J√∂rn Reinecke']
            for match in matches:
                full_name = f"{match[0]} {match[1]}"
                if full_name in known_names and full_name not in geschaeftsfuehrer:
                    geschaeftsfuehrer.append(full_name)
            
            if geschaeftsfuehrer:
                logger.info(f"üéØ T√¨m th·∫•y {len(geschaeftsfuehrer)} Gesch√§ftsf√ºhrer")
                return geschaeftsfuehrer
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract geschaeftsfuehrer: {str(e)}")
            return None
    
    def _extract_telefonnummer(self, page: Page) -> Optional[str]:
        """Extract Telefonnummer t·ª´ JSON-LD schema"""
        try:
            page_content = page.content()
            import re
            
            # CH·ªà l·∫•y t·ª´ JSON-LD schema ƒë·ªÉ ƒë·∫£m b·∫£o ch√≠nh x√°c
            # Pattern: "telephone" : "+49 40 238311200"
            json_ld_pattern = r'"telephone"\s*:\s*"([^"]+)"'
            json_ld_match = re.search(json_ld_pattern, page_content)
            
            if json_ld_match:
                telefon = json_ld_match.group(1).strip()
                logger.info(f"üéØ T√¨m th·∫•y Telefonnummer: {telefon}")
                return telefon
            
            # N·∫øu kh√¥ng c√≥ trong JSON-LD, kh√¥ng l·∫•y ƒë·ªÉ tr√°nh sai
            logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Telefonnummer trong JSON-LD schema")
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract telefonnummer: {str(e)}")
            return None
    
    def _extract_email(self, page: Page) -> Optional[str]:
        """Extract Email"""
        try:
            page_content = page.content()
            import re
            
            # Pattern: email address
            pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
            match = re.search(pattern, page_content)
            
            if match:
                email = match.group(1)
                logger.info(f"üéØ T√¨m th·∫•y Email: {email}")
                return email
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract email: {str(e)}")
            return None
    
    def _extract_website(self, page: Page) -> Optional[str]:
        """Extract Website"""
        try:
            page_content = page.content()
            import re
            
            # Pattern: website URL
            patterns = [
                r'(https?://[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'(www\.[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, page_content)
                if match:
                    website = match.group(1)
                    if 'northdata' not in website.lower():  # B·ªè qua northdata.de
                        logger.info(f"üéØ T√¨m th·∫•y Website: {website}")
                        return website
            
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói extract website: {str(e)}")
            return None
    
    def _save_html_to_magna_folder(self, page: Page, company_name: str, registernummer: str) -> str:
        """L∆∞u HTML v√†o th∆∞ m·ª•c data/companies/ v√† return filepath"""
        try:
            import re
            # L√†m s·∫°ch t√™n c√¥ng ty ƒë·ªÉ d√πng l√†m t√™n file
            clean_name = re.sub(r'[^\w\s-]', '', company_name).strip().replace(' ', '_')
            
            # ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c data/companies/
            companies_dir = os.path.join(
                os.path.dirname(os.path.dirname(__file__)), 
                'data',
                'companies'
            )
            os.makedirs(companies_dir, exist_ok=True)
            
            # T√™n file HTML v·ªõi t√™n c√¥ng ty
            html_filename = f"{clean_name}_{registernummer}_northdata.html"
            html_filepath = os.path.join(companies_dir, html_filename)
            
            # Ch·ªâ l·∫•y n·ªôi dung t·ª´ section b√™n trong main > div.anchor.content > section
            target_section = page.locator('main.ui.container > div.anchor.content > section').first
            if target_section and target_section.is_visible():
                html_content = target_section.inner_html()
                logger.info(f"üìÑ Target section content length: {len(html_content)} characters")
            else:
                # N·∫øu kh√¥ng t√¨m th·∫•y, l∆∞u full HTML ƒë·ªÉ debug
                html_content = page.content()
                logger.warning(f"‚ö†Ô∏è Target section not found, saving full HTML: {len(html_content)} characters")
            
            with open(html_filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logger.info(f"üíæ ƒê√£ l∆∞u HTML (ƒë√® l√™n file c≈©): {html_filepath}")
            
            # C≈©ng l∆∞u screenshot
            screenshot_filename = f"{clean_name}_{registernummer}_northdata.png"
            screenshot_filepath = os.path.join(companies_dir, screenshot_filename)
            
            page.screenshot(path=screenshot_filepath)
            logger.info(f"üì∏ ƒê√£ l∆∞u screenshot: {screenshot_filepath}")
            
            return html_filepath
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói save HTML to data folder: {str(e)}")
            return None
    
    def _save_html_debug(self, page: Page, company_name: str, registernummer: str, is_search_page: bool = False):
        """L∆∞u HTML ƒë·ªÉ debug v√† ph√¢n t√≠ch"""
        try:
            # T·∫°o th∆∞ m·ª•c Html_debug
            debug_dir = os.path.join(
                os.path.dirname(os.path.dirname(__file__)), 
                'Html_debug'
            )
            os.makedirs(debug_dir, exist_ok=True)
            
            # T·∫°o t√™n file
            safe_name = company_name.replace(' ', '_').replace('/', '_').replace('\\', '_')
            if is_search_page:
                filename = f"{safe_name}_{registernummer}_search_results.html"
            else:
                filename = f"{safe_name}_{registernummer}_company_page.html"
            
            filepath = os.path.join(debug_dir, filename)
            
            # L∆∞u HTML content v·ªõi error handling t·ªët h∆°n
            try:
                html_content = page.content()
                logger.info(f"üìÑ HTML content length: {len(html_content)} characters")
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                
                logger.info(f"üíæ ƒê√£ l∆∞u HTML debug: {filepath}")
                
            except Exception as html_error:
                logger.error(f"‚ùå L·ªói l∆∞u HTML content: {str(html_error)}")
                # Fallback: l∆∞u basic info
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(f"<!-- Error saving HTML: {str(html_error)} -->\n")
                    f.write(f"<html><body><h1>Error saving HTML</h1><p>{str(html_error)}</p></body></html>")
            
            # C≈©ng l∆∞u screenshot ƒë·ªÉ d·ªÖ debug
            try:
                screenshot_path = filepath.replace('.html', '.png')
                page.screenshot(path=screenshot_path)
                logger.info(f"üì∏ ƒê√£ l∆∞u screenshot: {screenshot_path}")
            except Exception as screenshot_error:
                logger.error(f"‚ùå L·ªói l∆∞u screenshot: {str(screenshot_error)}")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói save HTML debug: {str(e)}")


if __name__ == "__main__":
    import json
    
    # Load companies t·ª´ companies.json
    companies_file = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 
        'data', 
        'companies.json'
    )
    
    with open(companies_file, 'r', encoding='utf-8') as f:
        companies = json.load(f)
    
    scraper = NorthdataScraper(headless=False)  # Show browser for debugging
    
    # Test t·∫•t c·∫£ companies
    for i, company in enumerate(companies, 1):
        print(f"\n{'='*80}")
        print(f"TESTING COMPANY {i}/{len(companies)}: {company['company_name']}")
        print(f"{'='*80}")
        
        result = scraper.scrape_company(
            company['company_name'], 
            company['registernummer']
        )
        
        print("\nKET QUA EXTRACT:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        print(f"Da extract: {len(result)}/27 truong")
        print(f"{'='*80}\n")
        
        # ƒê·ª£i 3 gi√¢y tr∆∞·ªõc khi test c√¥ng ty ti·∫øp theo
        if i < len(companies):
            print("Doi 3 giay truoc khi test cong ty tiep theo...")
            time.sleep(3)
