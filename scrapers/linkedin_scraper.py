"""
LinkedIn Scraper
Scrapes company data from LinkedIn using API and web scraping
"""

import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from typing import Dict, Optional, List
import time
import logging
# from models.company_model import CompanyData  # Removed - not needed

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LinkedInScraper:
    """Scraper for LinkedIn"""
    
    def __init__(self):
        self.base_url = "https://www.linkedin.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Setup Chrome driver options
        self.chrome_options = Options()
        self.chrome_options.add_argument('--headless')  # Always headless on cloud
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        self.chrome_options.add_argument('--disable-gpu')
        self.chrome_options.add_argument('--disable-extensions')
        self.chrome_options.add_argument('--disable-plugins')
        self.chrome_options.add_argument('--disable-images')
        self.chrome_options.add_argument('--disable-web-security')
        self.chrome_options.add_argument('--disable-features=VizDisplayCompositor')
        self.chrome_options.add_argument('--remote-debugging-port=9222')
        self.chrome_options.add_argument('--window-size=1920,1080')
        # self.chrome_options.add_argument('--disable-javascript')  # LinkedIn needs JavaScript
        # Fix for Render.com - use unique user data directory
        import tempfile
        import os
        temp_dir = tempfile.mkdtemp()
        self.chrome_options.add_argument(f'--user-data-dir={temp_dir}')
        self.chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.chrome_options.add_experimental_option('useAutomationExtension', False)
    
    def scrape_company(self, company_name: str, registernummer: str) -> Dict:
        """
        Scrape company data from LinkedIn
        
        Args:
            company_name: Company name
            registernummer: HRB number
            
        Returns:
            Dict with scraped data
        """
        try:
            logger.info(f"Scraping LinkedIn for {company_name}")
            
            # Táº¡m thá»i return placeholder data
            # TODO: Implement actual LinkedIn scraping logic
            data = {
                'registernummer': registernummer,
                'mitarbeiter': None,  # Sáº½ extract tá»« LinkedIn company page
                'website': None,      # Sáº½ extract tá»« LinkedIn
                'email': None,        # Sáº½ extract tá»« LinkedIn
                'telefonnummer': None # Sáº½ extract tá»« LinkedIn
            }
            
            logger.info(f"âœ… LinkedIn placeholder data for {company_name}")
            return data
            
        except Exception as e:
            logger.error(f"âŒ Error scraping LinkedIn for {company_name}: {str(e)}")
            return {}
    
    def scrape_with_selenium(self, company_name: str, registernummer: str) -> Dict:
        """
        Scrape company data using Selenium (for dynamic content)
        
        Args:
            company_name: Company name
            registernummer: HRB number
            
        Returns:
            Dict with scraped data
        """
        try:
            logger.info(f"ğŸ” Scraping LinkedIn with Selenium for {company_name}")
            
            driver = webdriver.Chrome(options=self.chrome_options)
            
            # ThÃªm stealth script Ä‘á»ƒ áº©n automation
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # BÆ°á»›c 1: ÄÄƒng nháº­p LinkedIn
            logger.info("ğŸ” Logging into LinkedIn...")
            driver.get("https://www.linkedin.com/login")
            time.sleep(3)
            
            # Xá»­ lÃ½ cÃ¡c modal/popup ngay tá»« Ä‘áº§u
            self._dismiss_all_modals(driver)
            
            # Nháº­p email
            email_input = driver.find_element(By.ID, "username")
            email_input.send_keys("nguyenthaithanh101104@gmail.com")
            time.sleep(1)
            
            # Nháº­p password
            password_input = driver.find_element(By.ID, "password")
            password_input.send_keys("Nguyenthanh04")
            time.sleep(1)
            
            # Click login
            login_btn = driver.find_element(By.XPATH, "//button[@type='submit']")
            login_btn.click()
            time.sleep(5)
            
            # Xá»­ lÃ½ modal sau khi login
            self._dismiss_all_modals(driver)
            
            # BÆ°á»›c 2: TÃ¬m kiáº¿m cÃ´ng ty
            logger.info(f"ğŸ” Searching for company: {company_name}")
            search_input = driver.find_element(By.CSS_SELECTOR, "input[placeholder='Search']")
            search_input.clear()
            search_input.send_keys(company_name)
            search_input.send_keys(Keys.RETURN)  # Press Enter
            time.sleep(3)  # Äá»£i 3 giÃ¢y nhÆ° yÃªu cáº§u
            
            # Xá»­ lÃ½ modal sau khi search
            self._dismiss_all_modals(driver)
            
            # Debug: Log current URL vÃ  screenshot
            current_url = driver.current_url
            logger.info(f"ğŸ“ Current URL after search: {current_url}")
            
            # Screenshot Ä‘á»ƒ debug
            driver.save_screenshot("linkedin_search_debug.png")
            logger.info("ğŸ“¸ Saved screenshot: linkedin_search_debug.png")
            
            # BÆ°á»›c 3: Click "Companies" filter vá»›i xpath cá»¥ thá»ƒ
            logger.info("ğŸ¢ Clicking 'Companies' filter...")
            
            try:
                # Sá»­ dá»¥ng xpath cá»¥ thá»ƒ nhÆ° yÃªu cáº§u
                companies_btn = driver.find_element(By.XPATH, "//*[@id='search-reusables__filters-bar']/ul/li[3]/button")
                companies_btn.click()
                time.sleep(2)
                logger.info("âœ… Companies filter clicked successfully")
            except Exception as e:
                logger.warning(f"âš ï¸ Could not find Companies filter button: {e}")
                # Thá»­ cÃ¡c selector khÃ¡c lÃ m fallback
                companies_selectors = [
                    "//button[contains(text(), 'Companies')]",
                    "//button[contains(@class, 'artdeco-pill') and contains(text(), 'Companies')]",
                    "//button[contains(@class, 'search-reusables__filter-pill-button') and contains(text(), 'Companies')]"
                ]
                
                companies_btn = None
                for selector in companies_selectors:
                    try:
                        companies_btn = driver.find_element(By.XPATH, selector)
                        if companies_btn.is_displayed():
                            companies_btn.click()
                            time.sleep(2)
                            logger.info(f"âœ… Found Companies button with fallback selector: {selector}")
                            break
                    except:
                        continue
                
                if not companies_btn:
                    logger.warning("âš ï¸ Could not find any Companies filter button")
            
            # Tiáº¿p tá»¥c vá»›i viá»‡c tÃ¬m cÃ´ng ty Ä‘áº§u tiÃªn
            
            # BÆ°á»›c 4: Click vÃ o cÃ´ng ty Ä‘áº§u tiÃªn náº¿u tÃªn khá»›p
            logger.info("ğŸ¢ Looking for company with matching name...")
            
            # TÃ¬m táº¥t cáº£ cÃ¡c link cÃ´ng ty
            company_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='/company/']")
            
            if company_links:
                # Kiá»ƒm tra tÃªn cÃ´ng ty trong link Ä‘áº§u tiÃªn
                first_company_link = company_links[0]
                company_text = first_company_link.text.strip()
                logger.info(f"ğŸ“‹ Found company: {company_text}")
                
                # Kiá»ƒm tra xem tÃªn cÃ³ khá»›p vá»›i cÃ´ng ty Ä‘ang tÃ¬m khÃ´ng
                if company_name.lower() in company_text.lower() or company_text.lower() in company_name.lower():
                    logger.info(f"âœ… Company name matches! Clicking on: {company_text}")
                    first_company_link.click()
                    time.sleep(3)
                else:
                    logger.warning(f"âš ï¸ Company name doesn't match. Expected: {company_name}, Found: {company_text}")
                    # Váº«n click vÃ o cÃ´ng ty Ä‘áº§u tiÃªn
                    first_company_link.click()
                    time.sleep(3)
            else:
                logger.error("âŒ No company links found")
                return data
            time.sleep(5)
            
            # BÆ°á»›c 5: Xá»­ lÃ½ popup náº¿u cÃ³ (Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi _dismiss_all_modals)
            self._dismiss_all_modals(driver)
            
            # BÆ°á»›c 6: Click "About" tab
            logger.info("ğŸ“„ Clicking 'About' tab...")
            about_link = driver.find_element(By.XPATH, "//a[contains(@href, '/about/')]")
            about_link.click()
            time.sleep(3)
            
            # Xá»­ lÃ½ modal sau khi click About
            self._dismiss_all_modals(driver)
            
            # BÆ°á»›c 7: Láº¥y HTML cá»§a toÃ n bá»™ pháº§n About
            logger.info("ğŸ“„ Extracting full About section HTML...")
            
            # Láº¥y toÃ n bá»™ section About (bao gá»“m Overview, Website, Phone, Industry, Company size, Founded)
            about_section = driver.find_element(By.CSS_SELECTOR, "section.artdeco-card.org-page-details-module__card-spacing")
            about_html = about_section.get_attribute('outerHTML')
            
            logger.info(f"ğŸ“„ Retrieved full About section HTML ({len(about_html)} characters)")
            
            # Extract thÃ´ng tin cá»¥ thá»ƒ tá»« About section
            about_data = self._extract_about_data(about_section)
            
            # Extract data
            data = self._parse_selenium_data(driver, company_name, registernummer)
            data['about_html'] = about_html
            
            # Merge thÃ´ng tin tá»« About section
            data.update(about_data)
            
            logger.info("â³ Keeping browser open for 10 seconds to inspect...")
            time.sleep(10)  # Giá»¯ browser má»Ÿ Ä‘á»ƒ báº¡n xem
            
            driver.quit()
            
            logger.info(f"âœ… Successfully scraped {company_name} with Selenium")
            return data
            
        except Exception as e:
            logger.error(f"âŒ Error scraping {company_name} with Selenium: {str(e)}")
            return {}
    
    def _parse_company_data(self, soup: BeautifulSoup, company_name: str, registernummer: str) -> Dict:
        """Parse company data from HTML"""
        
        data = {
            'registernummer': registernummer,
            'mitarbeiter': self._extract_mitarbeiter(soup),
            'website': self._extract_website(soup),
            'email': self._extract_email(soup),
            'geschaeftsadresse': self._extract_geschaeftsadresse(soup)
        }
        
        return data
    
    def _parse_selenium_data(self, driver, company_name: str, registernummer: str) -> Dict:
        """Parse company data using Selenium"""
        
        data = {
            'registernummer': registernummer,
            'mitarbeiter': self._extract_mitarbeiter_selenium(driver),
            'website': self._extract_website_selenium(driver),
            'email': self._extract_email_selenium(driver),
            'geschaeftsadresse': self._extract_geschaeftsadresse_selenium(driver)
        }
        
        return data
    
    def _extract_mitarbeiter(self, soup: BeautifulSoup) -> int:
        """Extract number of employees from HTML"""
        # Implementation needed based on actual HTML structure
        return 14  # Placeholder for MAGNA
    
    def _extract_website(self, soup: BeautifulSoup) -> str:
        """Extract company website from HTML"""
        # Implementation needed based on actual HTML structure
        return "https://www.magna-real-estate.com"  # Placeholder
    
    def _extract_email(self, soup: BeautifulSoup) -> str:
        """Extract company email from HTML"""
        # Implementation needed based on actual HTML structure
        return "info@magna-real-estate.com"  # Placeholder
    
    def _extract_geschaeftsadresse(self, soup: BeautifulSoup) -> str:
        """Extract business address from HTML"""
        # Implementation needed based on actual HTML structure
        return "Hamburg, Deutschland"  # Placeholder
    
    def _extract_mitarbeiter_selenium(self, driver) -> int:
        """Extract number of employees using Selenium"""
        try:
            # Look for employee count element
            employee_element = driver.find_element(By.CSS_SELECTOR, "[data-test-id='employee-count']")
            employee_text = employee_element.text
            # Extract number from text like "14 employees"
            import re
            numbers = re.findall(r'\d+', employee_text)
            return int(numbers[0]) if numbers else 0
        except:
            return 0
    
    def _extract_website_selenium(self, driver) -> str:
        """Extract company website using Selenium"""
        try:
            website_element = driver.find_element(By.CSS_SELECTOR, "[data-test-id='company-website']")
            return website_element.get_attribute('href')
        except:
            return ""
    
    def _extract_email_selenium(self, driver) -> str:
        """Extract company email using Selenium"""
        try:
            email_element = driver.find_element(By.CSS_SELECTOR, "[data-test-id='company-email']")
            return email_element.text
        except:
            return ""
    
    def _extract_geschaeftsadresse_selenium(self, driver) -> str:
        """Extract business address using Selenium"""
        try:
            address_element = driver.find_element(By.CSS_SELECTOR, "[data-test-id='company-address']")
            return address_element.text
        except:
            return ""
    
    def _extract_about_data(self, about_section) -> Dict:
        """Extract specific data from About section"""
        data = {
            'website': None,
            'telefonnummer': None,
            'mitarbeiter': None,
            'industry': None,
            'founded': None
        }
        
        try:
            # Extract Website
            try:
                website_element = about_section.find_element(By.XPATH, "//dt[contains(., 'Website')]/following-sibling::dd//a")
                data['website'] = website_element.get_attribute('href')
                logger.info(f"âœ… Found website: {data['website']}")
            except:
                logger.info("â„¹ï¸ No website found")
            
            # Extract Phone
            try:
                phone_element = about_section.find_element(By.XPATH, "//dt[contains(., 'Phone')]/following-sibling::dd//a")
                data['telefonnummer'] = phone_element.get_attribute('href').replace('tel:', '')
                logger.info(f"âœ… Found phone: {data['telefonnummer']}")
            except:
                logger.info("â„¹ï¸ No phone found")
            
            # Extract Company size (sá»‘ nhÃ¢n viÃªn)
            try:
                size_element = about_section.find_element(By.XPATH, "//dt[contains(., 'Company size')]/following-sibling::dd")
                size_text = size_element.text
                # Extract sá»‘ tá»« text nhÆ° "51-200 employees"
                import re
                numbers = re.findall(r'\d+', size_text)
                if numbers:
                    # Láº¥y sá»‘ lá»›n nháº¥t (200 trong "51-200")
                    data['mitarbeiter'] = int(max(numbers))
                    logger.info(f"âœ… Found company size: {data['mitarbeiter']} employees")
            except:
                logger.info("â„¹ï¸ No company size found")
            
            # Extract Industry
            try:
                industry_element = about_section.find_element(By.XPATH, "//dt[contains(., 'Industry')]/following-sibling::dd")
                data['industry'] = industry_element.text.strip()
                logger.info(f"âœ… Found industry: {data['industry']}")
            except:
                logger.info("â„¹ï¸ No industry found")
            
            # Extract Founded year
            try:
                founded_element = about_section.find_element(By.XPATH, "//dt[contains(., 'Founded')]/following-sibling::dd")
                data['founded'] = founded_element.text.strip()
                logger.info(f"âœ… Found founded: {data['founded']}")
            except:
                logger.info("â„¹ï¸ No founded year found")
                
        except Exception as e:
            logger.error(f"Error extracting about data: {e}")
        
        return data
    
    def _dismiss_all_modals(self, driver):
        """
        Dismiss all possible modals, overlays, and popups on LinkedIn
        """
        logger.info("ğŸš« Dismissing all modals and overlays...")
        
        # Danh sÃ¡ch cÃ¡c selector Ä‘á»ƒ tÃ¬m vÃ  Ä‘Ã³ng modal/popup
        modal_selectors = [
            # Premium modal
            "button[aria-label='Dismiss']",
            "button[data-test-modal-close-btn]",
            ".artdeco-modal__dismiss",
            ".modal-dismiss",
            
            # X button variants
            "button[aria-label='Close']",
            "button[data-control-name='modal.dismiss']",
            
            # Premium upgrade modal
            ".premium-upsell-modal button[aria-label='Dismiss']",
            ".premium-upsell-modal .artdeco-modal__dismiss",
            
            # Network growth modal
            ".network-growth-modal button[aria-label='Dismiss']",
            ".network-growth-modal .artdeco-modal__dismiss",
            
            # Generic modal close buttons
            ".modal-close",
            ".close-button",
            ".dismiss-button",
            
            # LinkedIn specific
            "button[data-test-id='modal-close']",
            ".messaging-modal__dismiss",
            ".artdeco-toast-item__dismiss",
            
            # ESC key alternative - click outside modal
            ".artdeco-modal__overlay",
            ".modal-overlay"
        ]
        
        dismissed_count = 0
        
        # Thá»­ táº¥t cáº£ cÃ¡c selector
        for selector in modal_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    if element.is_displayed() and element.is_enabled():
                        try:
                            element.click()
                            dismissed_count += 1
                            logger.info(f"âœ… Dismissed modal with selector: {selector}")
                            time.sleep(0.5)  # Ngáº¯n delay giá»¯a cÃ¡c click
                        except:
                            # Náº¿u click khÃ´ng Ä‘Æ°á»£c, thá»­ JavaScript click
                            try:
                                driver.execute_script("arguments[0].click();", element)
                                dismissed_count += 1
                                logger.info(f"âœ… Dismissed modal with JS click: {selector}")
                                time.sleep(0.5)
                            except:
                                continue
            except:
                continue
        
        # Thá»­ nháº¥n ESC key Ä‘á»ƒ Ä‘Ã³ng modal
        try:
            from selenium.webdriver.common.keys import Keys
            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)
            time.sleep(0.5)
        except:
            pass
        
        # Thá»­ click vÃ o background Ä‘á»ƒ Ä‘Ã³ng modal
        try:
            driver.execute_script("""
                // Click on any modal overlay to close it
                const overlays = document.querySelectorAll('.artdeco-modal__overlay, .modal-overlay, .overlay');
                overlays.forEach(overlay => {
                    if (overlay.style.display !== 'none') {
                        overlay.click();
                    }
                });
                
                // Hide any visible modals
                const modals = document.querySelectorAll('.artdeco-modal, .modal, [role="dialog"]');
                modals.forEach(modal => {
                    if (modal.style.display !== 'none') {
                        modal.style.display = 'none';
                    }
                });
                
                // Remove any toast notifications
                const toasts = document.querySelectorAll('.artdeco-toast-item, .toast');
                toasts.forEach(toast => {
                    toast.remove();
                });
            """)
            logger.info("âœ… Executed JavaScript to dismiss modals")
        except Exception as e:
            logger.warning(f"âš ï¸ JavaScript modal dismissal failed: {e}")
        
        if dismissed_count > 0:
            logger.info(f"âœ… Total modals dismissed: {dismissed_count}")
        else:
            logger.info("â„¹ï¸ No modals found to dismiss")
        
        # Äá»£i má»™t chÃºt Ä‘á»ƒ Ä‘áº£m báº£o modal Ä‘Ã£ Ä‘Ã³ng hoÃ n toÃ n
        time.sleep(1)


if __name__ == "__main__":
    scraper = LinkedInScraper()
    
    print("\n" + "="*80)
    print("TESTING LINKEDIN SCRAPER")
    print("="*80 + "\n")
    
    # Test with MAGNA Real Estate using Selenium
    result = scraper.scrape_with_selenium("MAGNA Real Estate GmbH", "HRB182742")
    
    print("\n" + "="*80)
    print("SCRAPED DATA:")
    print("="*80)
    for key, value in result.items():
        print(f"  {key}: {value}")
    print("="*80 + "\n")
